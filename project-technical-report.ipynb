{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Song Recommender  \n",
    "by Braden Weber (blw22) and Kelsey Yen (kny4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project was to create a song recommender. Using the fundamentals from Units 6 and 12, we created a song recommender and validated the results based on how well it recommends songs to a genre-specific playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import pandas as pd## Goal  \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import math\n",
    "import json\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def num_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model\"\"\"\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we used is from the [Spotify Million Playlist Dataset Challenge](https://research.atspotify.com/the-million-playlist-dataset-remastered/). The data comes from sampling random existing Spotify playlists made by real users for Spotify's AI research (much of which focuses on recommendation systems).  \n",
    "Each playlist is a dictionary that includes data. The data given about each track is as follows:\n",
    "* **track_name** - the name of the track\n",
    "* **track_uri** - the Spotify URI of the track\n",
    "* **album_name** - the name of the track's album\n",
    "* **album_uri** - the Spotify URI of the album\n",
    "* **artist_name** - the name of the track's primary artist\n",
    "* **artist_uri** - the Spotify URI of track's primary artist\n",
    "* **duration_ms** - the duration of the track in milliseconds\n",
    "* **pos** - the position of the track in the playlist (zero-based)  \n",
    "\n",
    "Unlike other datasets we explored, the dataset does not include song attributes that can be compared to one another, things like tempo, genre, or danceability (although attributes are available using Spotify Web API). Since our model is trained on the occurence of songs in a certain type of playlist, the data is only used to identify unique songs in the dataset.  \n",
    "\n",
    "To use the dataset, it was unzipped then made into a datafram using this `for loop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many .json files to load in\n",
    "files_to_load = 1\n",
    "\n",
    "# Path to the folder containing the songs\n",
    "path = \"/home/blw22/Desktop/songs\"\n",
    "\n",
    "# Start a songs dataframe\n",
    "data = json.load(open(path + '/mpd.slice.0-999.json'))\n",
    "playlists = pd.DataFrame(data['playlists'])\n",
    "\n",
    "# Load in each json file and append it to the songs dataframe\n",
    "for i in range(1000, 1000 * (files_to_load - 1) + 1, 1000):\n",
    "    file = path + '/mpd.slice.' + str(i) + '-' + str(i + 999) + '.json'\n",
    "    print(i, end = ' ')\n",
    "    data = json.load(open(file))\n",
    "    playlists2 = pd.DataFrame(data['playlists'])\n",
    "    playlists = pd.concat([playlists, playlists2])\n",
    "# Reset index so there is not duplicate indices\n",
    "playlists = playlists.reset_index()\n",
    "\n",
    "# Drop all song data that isn't going to be used\n",
    "playlists = playlists.drop(labels=['collaborative', 'modified_at', 'num_tracks', 'num_albums', 'num_followers', 'num_edits', 'duration_ms', 'num_artists', 'description', 'name', 'pid'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the data easier to handle, we shortened playlists to 30 songs and deleted playlists with less than 30 songs. This ensures that every playlist has the same size and will work with tensors.  \n",
    "\n",
    "Next, we create the 10 training sets by pulling out the last ten songs in each playlist, respectively assigning them as the target song for each of the training sets (train_1 uses the song at index 21, train_2 uses the song at index 22, etc) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "playlists_train1 = []\n",
    "playlists_train2 = []\n",
    "playlists_train3 = []\n",
    "playlists_train4 = []\n",
    "playlists_train5 = []\n",
    "playlists_train6 = []\n",
    "playlists_train7 = []\n",
    "playlists_train8 = []\n",
    "playlists_train9 = []\n",
    "playlists_train10 = []\n",
    "\n",
    "for index, row in playlists.iterrows():\n",
    "    # Drop the song if it is less than 30 songs long\n",
    "    if (len(row['tracks']) < 30):\n",
    "        playlists = playlists.drop(labels=index, axis=0)\n",
    "        continue\n",
    "    # Take the last 10 songs and add them all to the 10 training sets\n",
    "    playlists_train1.append(row['tracks'][20])\n",
    "    playlists_train2.append(row['tracks'][21])\n",
    "    playlists_train3.append(row['tracks'][22])\n",
    "    playlists_train4.append(row['tracks'][23])\n",
    "    playlists_train5.append(row['tracks'][24])\n",
    "    playlists_train6.append(row['tracks'][25])\n",
    "    playlists_train7.append(row['tracks'][26])\n",
    "    playlists_train8.append(row['tracks'][27])\n",
    "    playlists_train9.append(row['tracks'][28])\n",
    "    playlists_train10.append(row['tracks'][29])\n",
    "    # Leave the first 20 songs of the playlist as our playlist data\n",
    "    playlists['tracks'][index] = row['tracks'][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs to turn each unique track into a number in order to work, so we put each track into a set, turn that set into a list, and use this list in the future to match songs up with their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13572"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks = set()\n",
    "for index, row in playlists.iterrows():\n",
    "    for track in row['tracks']:\n",
    "        tracks.add(track['track_name'] + ' - ' + track['artist_name'])\n",
    "for song in playlists_train1:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train2:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train3:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train4:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train5:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train6:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train7:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train8:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train9:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train10:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "tracks = list(tracks)\n",
    "\n",
    "len(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each song can be identified with a unique index, we go back through the playlists and replace the songs with their track index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "playlist_list = []\n",
    "\n",
    "for index, row in playlists.iterrows():\n",
    "    playlist = []\n",
    "    for track in row['tracks']:\n",
    "        playlist.append(tracks.index(track['track_name'] + ' - ' + track['artist_name']))\n",
    "    playlist_list.append(playlist)\n",
    "playlists_tensor = torch.tensor(playlist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_train1 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train1]\n",
    "playlists_train2 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train2]\n",
    "playlists_train3 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train3]\n",
    "playlists_train4 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train4]\n",
    "playlists_train5 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train5]\n",
    "playlists_train6 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train6]\n",
    "playlists_train7 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train7]\n",
    "playlists_train8 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train8]\n",
    "playlists_train9 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train9]\n",
    "playlists_train10 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train10]\n",
    "\n",
    "playlists_train1 = torch.tensor(playlists_train1)\n",
    "playlists_train2 = torch.tensor(playlists_train2)\n",
    "playlists_train3 = torch.tensor(playlists_train3)\n",
    "playlists_train4 = torch.tensor(playlists_train4)\n",
    "playlists_train5 = torch.tensor(playlists_train5)\n",
    "playlists_train6 = torch.tensor(playlists_train6)\n",
    "playlists_train7 = torch.tensor(playlists_train7)\n",
    "playlists_train8 = torch.tensor(playlists_train8)\n",
    "playlists_train9 = torch.tensor(playlists_train9)\n",
    "playlists_train10 = torch.tensor(playlists_train10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shapes of our tensors:\n",
    "\n",
    "Training shape should equal the number of playlists.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists_train1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playlists shape shows that there are 20 songs in every playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the embedding of each song, we used the number of unique songs and the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13572"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_songs = len(tracks)\n",
    "unique_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13572, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = unique_songs\n",
    "emb_dim = 50\n",
    "embedder = nn.Embedding(n_vocab, emb_dim)\n",
    "embedder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678600"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we inserted tensor of playlists into `embedder` to create a tensor that has the embeddings for each song in a given playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 20, 50])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_play = embedder(playlists_tensor)\n",
    "emb_play.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model architecture is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"model.png\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The playlist tensor on top holds all the playlists and their 20 songs. Each playlist can be put into the model to output the top 15 songs recommended to add to the playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the MLPs. Each corresponds to the new parameter added:\n",
    "* MLP_mean: The input and output size is the embedding dimension. Mean is a tensor of length 50 and is the average of the rows for each embedding for the songs (an average of all the songs). \n",
    "* MLP_variance: This runs after the variance of each embedding in the playlist is added into the model. This also has an input and output size equal to the embedding dimension.\n",
    "* MLP_head: This has three layers that have outputs sized respectively at 100, 50, and the amount of unique tracks. We keep the layer before the final one at 50 because if it is too large, there are too many parameters to keep track of and the kernel dies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10150\n",
      "10150\n",
      "702322\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1 = 100\n",
    "hidden_layer2 = 50\n",
    "mlp_mean = nn.Sequential(\n",
    "    nn.Linear(in_features=emb_dim, out_features=hidden_layer1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=hidden_layer1, out_features=emb_dim)\n",
    ")\n",
    "mlp_variance = nn.Sequential(\n",
    "    nn.Linear(in_features=emb_dim, out_features=hidden_layer1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=hidden_layer1, out_features=emb_dim)\n",
    ")\n",
    "mlp_head = nn.Sequential(\n",
    "    nn.Linear(in_features=emb_dim, out_features = hidden_layer1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=hidden_layer1, out_features = hidden_layer2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=hidden_layer2, out_features = unique_songs)\n",
    ")\n",
    "print(num_parameters(mlp_mean))\n",
    "print(num_parameters(mlp_variance))\n",
    "print(num_parameters(mlp_head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function runs our model, which takes in the mean and variance of a plyalist. It returns a large array of scores for each possible song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(mean, var):\n",
    "    output1 = mlp_mean(mean)\n",
    "    output1 += var + mean\n",
    "    output2 = mlp_variance(output1)\n",
    "    output2 += output1\n",
    "    return mlp_head(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models use an optimizer to step through the gradients and zero them out afterward. Pytorch provides a nice function to wrap the optimizing process into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "\n",
    "optimizer_mean = torch.optim.SGD(mlp_mean.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer_variance = torch.optim.SGD(mlp_variance.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer_head = torch.optim.SGD(mlp_head.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer_embedding = torch.optim.SGD(embedder.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a playlist through the model, we need someway to convert the playlist into its mean and variance tensors. This funciton takes in a playlist and the list of all song embeddings and finds the mean and variance to input into the model, then it runs the model and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(playlist_index, emb):\n",
    "    songs = emb[playlist_index]\n",
    "    \n",
    "    mean = torch.tensor([0.0 for i in range(emb_dim)])\n",
    "    var = torch.tensor([0.0 for i in range(emb_dim)])\n",
    "\n",
    "    # calculate mean\n",
    "    for j, emb in enumerate(songs):\n",
    "        mean += emb\n",
    "    mean /= len(songs)\n",
    "\n",
    "    # calculate variance\n",
    "    for j, emb in enumerate(songs):\n",
    "        var += (emb - mean) * (emb - mean)\n",
    "    var /= len(songs)\n",
    "    for j, _ in enumerate(var):\n",
    "        var[j] = math.sqrt(var[j])\n",
    "\n",
    "    # Run through the model\n",
    "    return my_model(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, this function enumerates through every playlist in the list \"playlists\", runs it through the model, calculates the loss of the model, and then backpropogates on the sum of all losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training set is 1, 2, or 3\n",
    "def train(playlists, training_set):\n",
    "    loss_total = torch.tensor([0.])\n",
    "    \n",
    "    for i, songs in enumerate(playlists):\n",
    "                \n",
    "        # Run through the model\n",
    "        output = get_recommendations(i, playlists)\n",
    "\n",
    "        # Select training set\n",
    "        if training_set == 1:\n",
    "            answer = playlists_train1[i].reshape((1))\n",
    "        if training_set == 2:\n",
    "            answer = playlists_train2[i].reshape((1))\n",
    "        if training_set == 3:\n",
    "            answer = playlists_train3[i].reshape((1))\n",
    "        if training_set == 4:\n",
    "            answer = playlists_train4[i].reshape((1))\n",
    "        if training_set == 5:\n",
    "            answer = playlists_train5[i].reshape((1))\n",
    "        if training_set == 6:\n",
    "            answer = playlists_train6[i].reshape((1))\n",
    "        if training_set == 7:\n",
    "            answer = playlists_train7[i].reshape((1))\n",
    "        if training_set == 8:\n",
    "            answer = playlists_train8[i].reshape((1))\n",
    "        if training_set == 9:\n",
    "            answer = playlists_train9[i].reshape((1))\n",
    "        if training_set == 10:\n",
    "            answer = playlists_train10[i].reshape((1))\n",
    "        \n",
    "        # Calculate loss\n",
    "        output = output.reshape((1, unique_songs))\n",
    "        # Cross-entropy takes two argument: the huge array of probailities \n",
    "        # of each unique song and the index of the target song.\n",
    "        loss_total += F.cross_entropy(output, answer, reduction='none')\n",
    "\n",
    "    # Goes backward down the gradient to update each parameter in the model\n",
    "    loss_total.backward(retain_graph=True)\n",
    "    \n",
    "    # Step along gradient\n",
    "    optimizer_mean.step()\n",
    "    optimizer_variance.step()\n",
    "    optimizer_head.step()\n",
    "    optimizer_embedding.step()\n",
    "\n",
    "    # IMPORTANT: Zero the gradient every time the loss is used to back propogate to create the gradients. \n",
    "    # This saves memory space and keeps the kernel alive.\n",
    "    optimizer_mean.zero_grad()\n",
    "    optimizer_variance.zero_grad()\n",
    "    optimizer_head.zero_grad()\n",
    "    optimizer_embedding.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization of the model comes from adjusting the following 4 parameters:\n",
    "* **learning rate**: how fast the weights change as it goes backwards along the gradient in the mlp  \n",
    "* **files**: changes how many playlists are loaded in  \n",
    "* **training set**: how many traning sets the model runs through  \n",
    "* **epochs**: how many times the whole process is repeated  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle training, we have the two parameters epochs and training_sets. Epochs determines how many times the full model is run, and the training_sets determines how many of the 10 training sets it runs through per epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2, Sets: 1\n",
      "\n",
      "epoch 0\n",
      "set 1, \n",
      "epoch 1\n",
      "set 1, \n"
     ]
    }
   ],
   "source": [
    "# currently, if either of these are set to one, the loss.backwards() function gives an error\n",
    "training_sets = 1 \n",
    "epochs = 2\n",
    "\n",
    "print('Epochs: {}, Sets: {}'.format(epochs, training_sets))\n",
    "print()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch ' + str(epoch))\n",
    "    for i in range(1 , training_sets + 1):\n",
    "        train(emb_play, i)\n",
    "        print('set {},'.format(i), end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, our model currently relies on the \"retain_grad\" flag on the loss.backward() function. We believe this is causing errors, but we can't find a way around it. Because of this, the songs recommended come out a little funky, as explained in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This funciton shows the top 15 songs that the model recommends. It takes in the index of the plalist (index 25 is a country playlist). It shows what each training set had as it's training target and then the actual songs it recommended as well as its confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training target 1: Soak Up The Sun - Sheryl Crow  \n",
      "Training target 2: Where Is The Love? - The Black Eyed Peas  \n",
      "Training target 3: Stacy's Mom - Bowling For Soup  \n",
      "Training target 4: Just The Girl - The Click Five  \n",
      "Training target 5: Yo (Excuse Me Miss) - Chris Brown  \n",
      "Training target 6: Year 3000 - Jonas Brothers  \n",
      "Training target 7: Lip Gloss - Lil Mama  \n",
      "Training target 8: Everytime We Touch - Radio Edit - Cascada  \n",
      "Training target 9: Whatcha Say - Jason Derulo  \n",
      "Training target 10: Miss Independent - Ne-Yo  \n",
      "\n",
      "0.000114, Overcome - As I Lay Dying  \n",
      "0.000113, Young & Gettin' It - feat. Kirko Bangz - Meek Mill  \n",
      "0.000113, Bang Bang Bang - Russ Chimes Remix - Mark Ronson  \n",
      "0.000112, All Over The Road - Easton Corbin  \n",
      "0.000112, Untitled - Matt Corby  \n",
      "0.000108, Two Step - Mel Waiters  \n",
      "0.000107, Blame It - Jamie Foxx  \n",
      "0.000106, When I See Ya (feat. Fetty Wap) - Ty Dolla $ign  \n",
      "0.000106, When The Saints Go To Worship (feat. Kelly Price) - Donald Lawrence  \n",
      "0.000106, Astral Weeks - Van Morrison  \n",
      "0.000105, 1980 - Dirt Nasty  \n",
      "0.000105, Debaser - Pixies  \n",
      "0.000105, 679 (feat. Remy Boyz) - Fetty Wap  \n",
      "0.000105, Twist And Shout - Remastered 2009 - The Beatles  \n",
      "0.000105, I Got 5 On It - Feat. Michael Marshall - Luniz  \n"
     ]
    }
   ],
   "source": [
    "def show_recommendation(playlist_index):\n",
    "    top_fifteen = get_recommendations(playlist_index, emb_play).softmax(dim=0).topk(15)\n",
    "\n",
    "    recommend = [[tracks[song], ] for song in top_fifteen.indices]\n",
    "    probability = [prop for prop in top_fifteen.values]\n",
    "    print('Training target 1: ' + tracks[playlists_train1[playlist_index].item()] + '  ')\n",
    "    print('Training target 2: ' + tracks[playlists_train2[playlist_index].item()] + '  ')\n",
    "    print('Training target 3: ' + tracks[playlists_train3[playlist_index].item()] + '  ')\n",
    "    print('Training target 4: ' + tracks[playlists_train4[playlist_index].item()] + '  ')\n",
    "    print('Training target 5: ' + tracks[playlists_train5[playlist_index].item()] + '  ')\n",
    "    print('Training target 6: ' + tracks[playlists_train6[playlist_index].item()] + '  ')\n",
    "    print('Training target 7: ' + tracks[playlists_train7[playlist_index].item()] + '  ')\n",
    "    print('Training target 8: ' + tracks[playlists_train8[playlist_index].item()] + '  ')\n",
    "    print('Training target 9: ' + tracks[playlists_train9[playlist_index].item()] + '  ')\n",
    "    print('Training target 10: ' + tracks[playlists_train10[playlist_index].item()] + '  ')\n",
    "    print()\n",
    "    for song, prob in zip(recommend, probability):\n",
    "        print('{:.6f}, {}  '.format(prob.item(), song[0]))\n",
    "\n",
    "show_recommendation(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the model, we analyzed the number of country songs were recommended (generally most distinguishable genre)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the country playlist we found in position 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_playlist(num):\n",
    "    raw_playlist = playlist_list[num][:]\n",
    "    for i, song in enumerate(raw_playlist):\n",
    "        raw_playlist[i] = tracks[song]\n",
    "    return raw_playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Drink A Beer - Luke Bryan',\n",
       " 'Crash My Party - Luke Bryan',\n",
       " 'Country Girl (Shake It For Me) - Luke Bryan',\n",
       " 'Drunk On You - Luke Bryan',\n",
       " \"That's My Kind Of Night - Luke Bryan\",\n",
       " \"If Heaven Wasn't So Far Away - Justin Moore\",\n",
       " 'Cowboys and Angels - Dustin Lynch',\n",
       " \"Where It's At - Dustin Lynch\",\n",
       " 'Little Toy Guns - Carrie Underwood',\n",
       " 'Two Black Cadillacs - Carrie Underwood',\n",
       " 'Good Girl - Carrie Underwood',\n",
       " 'Blown Away - Carrie Underwood',\n",
       " 'I Drive Your Truck - Lee Brice',\n",
       " 'Drinking class - Lee Brice',\n",
       " \"I Don't Dance - Lee Brice\",\n",
       " \"Beachin' - Jake Owen\",\n",
       " 'Barefoot Blue Jean Night - Jake Owen',\n",
       " 'Cop Car - Keith Urban',\n",
       " 'Somewhere In My Car - Keith Urban',\n",
       " 'Six Foot Town - Big & Rich']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_playlist(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found example playlists for three popular genres to use in validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example playlist(25) - country**\n",
    "\n",
    "Training target 1: Caught Up In The Moment - Big & Rich  \n",
    "Training target 2: 8th Of November - Album Version w/o Intro - Big & Rich  \n",
    "Training target 3: Save A Horse (Ride A Cowboy) - Big & Rich  \n",
    "Training target 4: Rollin' (The Ballad Of Big & Rich) - The Ballad Of Big & Rich Album Version - Big & Rich  \n",
    "Training target 5: Holy Water - Big & Rich  \n",
    "Training target 6: Something in the Water - Carrie Underwood  \n",
    "Training target 7: Cowboy Casanova - Carrie Underwood  \n",
    "Training target 8: One Way Ticket - Carrie Underwood  \n",
    "Training target 9: Thank God For Hometowns - Carrie Underwood  \n",
    "Training target 10: Girl In A Country Song - Maddie & Tae  \n",
    "\n",
    "**Example playlist(7) - rap/hip-hop**\n",
    "\n",
    "Training target 1: I Don't Fuck With You - Big Sean  \n",
    "Training target 2: Drowning (feat. Kodak Black) - A Boogie Wit da Hoodie  \n",
    "Training target 3: goosebumps - Travis Scott  \n",
    "Training target 4: STFU - mansionz  \n",
    "Training target 5: Exposed - Russ  \n",
    "Training target 6: Slippery (feat. Gucci Mane) - Migos  \n",
    "Training target 7: m.A.A.d city - Kendrick Lamar  \n",
    "Training target 8: Yellow - Aminé  \n",
    "Training target 9: Juke Jam (feat. Justin Bieber & Towkio) - Chance The Rapper  \n",
    "Training target 10: Bodak Yellow - Cardi B\n",
    "\n",
    "**Example playlist(83) - Christmas**\n",
    "\n",
    "Training target 1: Run Rudolph Run - Single Version - Chuck Berry  \n",
    "Training target 2: Santa Claus Is Comin' to Town - Single Version - Bruce Springsteen  \n",
    "Training target 3: Linus And Lucy - Vince Guaraldi Trio  \n",
    "Training target 4: Christmas Time Is Here - Vocal - Vince Guaraldi Trio  \n",
    "Training target 5: Do They Know It's Christmas? - 1984 Version - Band Aid  \n",
    "Training target 6: All I Want for Christmas Is You - Mariah Carey  \n",
    "Training target 7: Santa Claus Go Straight To The Ghetto - James Brown  \n",
    "Training target 8: Joy To The World - Sufjan Stevens  \n",
    "Training target 9: Little Drummer Boy/Silent Night/Auld Lang Syne - Extended Version - Jimi Hendrix  \n",
    "Training target 10: Christmas All Over Again - Tom Petty and the Heartbreakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatley, the model appears to be broken. When it trains, it gains more and more confidence in its top 15 songs - for every playlist. That is to say, the longer it trains, the less it distinguishes between playlist and the more confidence it has in the same 15 songs. The reason for this behavior is unknown, but we assume it has to do with the loss.backward() function while we train. We currently have to use retain_graph=True because it gives errors otherwise, and we assume if we could get rid of this flag the model would work a lot better.\n",
    "\n",
    "Because of this, even though we have a way to validate the model, we haven't used this method much because the output is so obviously wrong. If it gave a different set of songs for each playlist, it would be easier to check the recommendations for these three albums and see if they match up to the genre. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had an earlier version of the model that concatenated the mean, variance, and target song in order to train rather than going through mean and variance sequentially. We moved away from this architecture once we realized that gradient information was not retained when we did this because concatenating tensors loses the gradient information in them. So our first improvement was to change models.\n",
    "\n",
    "When we changed to the new model, we initially fed it a song that was meant to go into the playlist to help it along in training, and when we did this, we tried to optimize by changing four parameters - amount of data loaded in, learning rate, training sets used, and epochs.\n",
    "\n",
    "Eventually, it hit us that the end result of the model can't accept a song that is supposed to go into the playlist because the whole point is to recommend a song using only the playlist itself. Once we made this adjustement, the model started acting funny. It recommended the same sets of songs for every playlist. Unfortunately, we could not figure out where this quick came from, and as a result, we did not have a meaningful way to optimize it.\n",
    "\n",
    "If we would ever get the model working, we definitely could load in more datasets to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other ways to set up a sequential model. We could have set up the mlp to calculate the output directly instead of calculating the change in its input. We also could have added more or less layers. We took into account both mean and variance, but it could be viable to use only one.  \n",
    "\n",
    "We also could have used pytorch's model of an LSTM to make our model, but the documentation online turned out to be sorely confusing and unhelpful. For someone who has experience using the pytorch LSTM, this could possibly be a better way to run our architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, our model does not perform the way we intended it to. It recommends similar songs for every playlist, and the longer it trains, the more it locks in its favorite songs no matter what the playlist is.  \n",
    "\n",
    "In the future, greater research on how to loop through backpropogation without remembering the gradiant graph would probably be necessary to make the model work. In the 90 minutes of research the team did, we could not find the answer to this question. Since we have found no other flaw in the architecture, our conclusion is that this problem is the cause of our unhelpful recommendations.\n",
    "\n",
    "To conclude, this project is open for future development and research."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8f4b6a4c38f2085446f7f3f791a848eb62f1444c2c1b7234c3e9244444f9b89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
