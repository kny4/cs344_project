{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Song Recommender  \n",
    "by Braden Weber (blw22) and Kelsey Yen (kny4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project was to create a song recommender, based on Spotify's open-challenge to perfect their playlist recommendation system. Using the fundamentals from Units 6 and 12, we created a song recommender and validated the results based on how well it recommends songs to a genre-specific playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of our model is based on PyTorch's torch.nn module, which is how we constructed the sequential model and embeddings and calculating and handling gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import pandas as pd  \n",
    "import math\n",
    "import json\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def num_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model\"\"\"\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we used is from the [Spotify Million Playlist Dataset Challenge](https://research.atspotify.com/the-million-playlist-dataset-remastered/). The data comes from sampling random existing Spotify playlists made by real users for Spotify's AI research (much of which focuses on recommendation systems). Each playlist is a dictionary that includes data anout both the playlist and the tracks in the playlist. The data given about each track is as follows:\n",
    "* **track_name** - the name of the track\n",
    "* **track_uri** - the Spotify URI of the track\n",
    "* **album_name** - the name of the track's album\n",
    "* **album_uri** - the Spotify URI of the album\n",
    "* **artist_name** - the name of the track's primary artist\n",
    "* **artist_uri** - the Spotify URI of track's primary artist\n",
    "* **duration_ms** - the duration of the track in milliseconds\n",
    "* **pos** - the position of the track in the playlist (zero-based)  \n",
    "\n",
    "Unlike other datasets we explored, the dataset does not include song attributes that can be compared to one another, things like tempo, genre, or danceability (although attributes are available using Spotify Web API). Since our model is trained on the occurence of songs in a certain type of playlist, the data is only used to identify unique songs in the dataset.  \n",
    "\n",
    "To use the dataset, it was unzipped then made into a dataframe using this `for loop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many .json files to load in\n",
    "files_to_load = 1\n",
    "\n",
    "# Path to the folder containing the songs\n",
    "path = \"/home/blw22/Desktop/songs\"\n",
    "\n",
    "# Start a songs dataframe\n",
    "data = json.load(open(path + '/mpd.slice.0-999.json'))\n",
    "playlists = pd.DataFrame(data['playlists'])\n",
    "\n",
    "# Load in each json file and append it to the songs dataframe\n",
    "for i in range(1000, 1000 * (files_to_load - 1) + 1, 1000):\n",
    "    file = path + '/mpd.slice.' + str(i) + '-' + str(i + 999) + '.json'\n",
    "    print(i, end = ' ')\n",
    "    data = json.load(open(file))\n",
    "    playlists2 = pd.DataFrame(data['playlists'])\n",
    "    playlists = pd.concat([playlists, playlists2])\n",
    "# Reset index so there is not duplicate indices\n",
    "playlists = playlists.reset_index()\n",
    "\n",
    "# Drop all song data that isn't going to be used\n",
    "playlists = playlists.drop(labels=['collaborative', 'modified_at', 'num_tracks', 'num_albums', 'num_followers', 'num_edits', 'duration_ms', 'num_artists', 'description', 'name', 'pid'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the data easier to handle, we shortened playlists to 30 songs and deleted playlists with less than 30 songs. This ensured that every playlist had the same size and could be compiled into tensors.  \n",
    "\n",
    "Next, we created the 10 training sets by pulling out the last ten songs in each playlist, respectively assigning them as the target song for each of the training sets (train_1 uses the song at index 21, train_2 uses the song at index 22, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "playlists_train1 = []\n",
    "playlists_train2 = []\n",
    "playlists_train3 = []\n",
    "playlists_train4 = []\n",
    "playlists_train5 = []\n",
    "playlists_train6 = []\n",
    "playlists_train7 = []\n",
    "playlists_train8 = []\n",
    "playlists_train9 = []\n",
    "playlists_train10 = []\n",
    "\n",
    "for index, row in playlists.iterrows():\n",
    "    # Drop the song if it is less than 30 songs long\n",
    "    if (len(row['tracks']) < 30):\n",
    "        playlists = playlists.drop(labels=index, axis=0)\n",
    "        continue\n",
    "    # Take the last 10 songs and add them all to the 10 training sets\n",
    "    playlists_train1.append(row['tracks'][20])\n",
    "    playlists_train2.append(row['tracks'][21])\n",
    "    playlists_train3.append(row['tracks'][22])\n",
    "    playlists_train4.append(row['tracks'][23])\n",
    "    playlists_train5.append(row['tracks'][24])\n",
    "    playlists_train6.append(row['tracks'][25])\n",
    "    playlists_train7.append(row['tracks'][26])\n",
    "    playlists_train8.append(row['tracks'][27])\n",
    "    playlists_train9.append(row['tracks'][28])\n",
    "    playlists_train10.append(row['tracks'][29])\n",
    "    # Leave the first 20 songs of the playlist as our playlist data\n",
    "    playlists['tracks'][index] = row['tracks'][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs to turn each unique track into a number in order to work, so we put each track into a set, turn that set into a list, and use this list in the future to match songs up with their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13572"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks = set()\n",
    "for index, row in playlists.iterrows():\n",
    "    for track in row['tracks']:\n",
    "        tracks.add(track['track_name'] + ' - ' + track['artist_name'])\n",
    "for song in playlists_train1:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train2:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train3:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train4:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train5:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train6:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train7:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train8:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train9:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "for song in playlists_train10:\n",
    "    tracks.add(song['track_name'] + ' - ' + song['artist_name'])\n",
    "tracks = list(tracks)\n",
    "\n",
    "len(tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each song can be identified with a unique index, we go back through the playlists and replaced the songs with their track index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "playlist_list = []\n",
    "\n",
    "for index, row in playlists.iterrows():\n",
    "    playlist = []\n",
    "    for track in row['tracks']:\n",
    "        playlist.append(tracks.index(track['track_name'] + ' - ' + track['artist_name']))\n",
    "    playlist_list.append(playlist)\n",
    "playlists_tensor = torch.tensor(playlist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_train1 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train1]\n",
    "playlists_train2 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train2]\n",
    "playlists_train3 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train3]\n",
    "playlists_train4 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train4]\n",
    "playlists_train5 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train5]\n",
    "playlists_train6 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train6]\n",
    "playlists_train7 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train7]\n",
    "playlists_train8 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train8]\n",
    "playlists_train9 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train9]\n",
    "playlists_train10 = [tracks.index(song['track_name'] + ' - ' + song['artist_name']) for song in playlists_train10]\n",
    "\n",
    "playlists_train1 = torch.tensor(playlists_train1)\n",
    "playlists_train2 = torch.tensor(playlists_train2)\n",
    "playlists_train3 = torch.tensor(playlists_train3)\n",
    "playlists_train4 = torch.tensor(playlists_train4)\n",
    "playlists_train5 = torch.tensor(playlists_train5)\n",
    "playlists_train6 = torch.tensor(playlists_train6)\n",
    "playlists_train7 = torch.tensor(playlists_train7)\n",
    "playlists_train8 = torch.tensor(playlists_train8)\n",
    "playlists_train9 = torch.tensor(playlists_train9)\n",
    "playlists_train10 = torch.tensor(playlists_train10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shapes of our tensors:\n",
    "\n",
    "The training shape equals the number of playlists.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists_train1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The playlists shape shows that there are 20 songs in every playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playlists_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the embedding of each song, we used the number of unique songs and the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13572"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_songs = len(tracks)\n",
    "unique_songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our dataset did not contain attributes of the songs, we gave the song embeddings a large number, in the hopes that it was enough for the model to recognize the genre-similarity between songs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13572, 50])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = unique_songs\n",
    "emb_dim = 50\n",
    "embedder = nn.Embedding(n_vocab, emb_dim)\n",
    "embedder.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678600"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_parameters(embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we inserted the tensor of playlists into `embedder` to create a tensor that has the embeddings for each song in a given playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 20, 50])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_play = embedder(playlists_tensor)\n",
    "emb_play.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model architecture is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"model.png\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The playlist tensor on top holds all the playlists and their 20 songs. Each playlist can be put into the model to output the top 15 songs recommended to add to the playlist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about the MLPs. Each corresponds to the new parameter added:\n",
    "* MLP_mean: The input and output size is the embedding dimension. Mean is a tensor of length 50 and is the average of the rows for each embedding for the songs (an average of all the songs). \n",
    "* MLP_variance: This runs after the variance of each embedding in the playlist is added into the model. This also has an input and output size equal to the embedding dimension.\n",
    "* MLP_head: This has three layers that have outputs sized respectively at 100, 50, and the amount of unique tracks. We keep the layer before the final one at 50 because if it is too large, there are too many parameters to keep track of and the kernel dies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10150\n",
      "10150\n",
      "702322\n"
     ]
    }
   ],
   "source": [
    "hidden_layer1 = 100\n",
    "hidden_layer2 = 50\n",
    "mlp_mean = nn.Sequential(\n",
    "    nn.Linear(in_features=emb_dim, out_features=hidden_layer1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=hidden_layer1, out_features=emb_dim)\n",
    ")\n",
    "mlp_variance = nn.Sequential(\n",
    "    nn.Linear(in_features=emb_dim, out_features=hidden_layer1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=hidden_layer1, out_features=emb_dim)\n",
    ")\n",
    "mlp_head = nn.Sequential(\n",
    "    nn.Linear(in_features=emb_dim, out_features = hidden_layer1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=hidden_layer1, out_features = hidden_layer2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=hidden_layer2, out_features = unique_songs)\n",
    ")\n",
    "print(num_parameters(mlp_mean))\n",
    "print(num_parameters(mlp_variance))\n",
    "print(num_parameters(mlp_head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function ran our model, which takes in the mean and variance of a playlist and returns a large array of scores for each possible song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(mean, var):\n",
    "    output1 = mlp_mean(mean)\n",
    "    output1 += var + mean\n",
    "    output2 = mlp_variance(output1)\n",
    "    output2 += output1\n",
    "    return mlp_head(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model used an optimizer to step through the gradients and zero them out afterward. Pytorch provides a nice function to wrap the optimizing process into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "optimizer_mean = torch.optim.SGD(mlp_mean.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer_variance = torch.optim.SGD(mlp_variance.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer_head = torch.optim.SGD(mlp_head.parameters(), lr=learning_rate, momentum=0.9)\n",
    "optimizer_embedding = torch.optim.SGD(embedder.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a playlist through the model, we needed to convert the playlist into its mean and variance tensors. The following function takes in a playlist and the list of all song embeddings and finds the mean and variance to input into the model. Then it runs the model and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(playlist_index, emb):\n",
    "    songs = emb[playlist_index]\n",
    "    \n",
    "    mean = torch.tensor([0.0 for i in range(emb_dim)])\n",
    "    var = torch.tensor([0.0 for i in range(emb_dim)])\n",
    "\n",
    "    # calculate mean\n",
    "    for j, emb in enumerate(songs):\n",
    "        mean += emb\n",
    "    mean /= len(songs)\n",
    "\n",
    "    # calculate variance\n",
    "    for j, emb in enumerate(songs):\n",
    "        var += (emb - mean) * (emb - mean)\n",
    "    var /= len(songs)\n",
    "    for j, _ in enumerate(var):\n",
    "        var[j] = math.sqrt(var[j])\n",
    "\n",
    "    # Run through the model\n",
    "    return my_model(mean, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, the following function enumerates through every playlist in the list \"playlists\", runs it through the model, calculates the loss of the model, and then backpropogates on the sum of all losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training set is 1, 2, or 3\n",
    "def train(playlists, training_set):\n",
    "    loss_total = torch.tensor([0.])\n",
    "    \n",
    "    for i, songs in enumerate(playlists):\n",
    "                \n",
    "        # Run through the model\n",
    "        output = get_recommendations(i, playlists)\n",
    "\n",
    "        # Select training set\n",
    "        if training_set == 1:\n",
    "            answer = playlists_train1[i].reshape((1))\n",
    "        if training_set == 2:\n",
    "            answer = playlists_train2[i].reshape((1))\n",
    "        if training_set == 3:\n",
    "            answer = playlists_train3[i].reshape((1))\n",
    "        if training_set == 4:\n",
    "            answer = playlists_train4[i].reshape((1))\n",
    "        if training_set == 5:\n",
    "            answer = playlists_train5[i].reshape((1))\n",
    "        if training_set == 6:\n",
    "            answer = playlists_train6[i].reshape((1))\n",
    "        if training_set == 7:\n",
    "            answer = playlists_train7[i].reshape((1))\n",
    "        if training_set == 8:\n",
    "            answer = playlists_train8[i].reshape((1))\n",
    "        if training_set == 9:\n",
    "            answer = playlists_train9[i].reshape((1))\n",
    "        if training_set == 10:\n",
    "            answer = playlists_train10[i].reshape((1))\n",
    "        \n",
    "        # Calculate loss\n",
    "        output = output.reshape((1, unique_songs))\n",
    "        # Cross-entropy takes two argument: the huge array of probailities \n",
    "        # of each unique song and the index of the target song.\n",
    "        loss_total += F.cross_entropy(output, answer, reduction='none')\n",
    "\n",
    "    # Goes backward down the gradient to update each parameter in the model\n",
    "    loss_total.backward(retain_graph=True)\n",
    "    \n",
    "    # Step along gradient\n",
    "    optimizer_mean.step()\n",
    "    optimizer_variance.step()\n",
    "    optimizer_head.step()\n",
    "    optimizer_embedding.step()\n",
    "\n",
    "    # IMPORTANT: Zero the gradient every time the loss is used to back propogate to create the gradients. \n",
    "    # This saves memory space and keeps the kernel alive.\n",
    "    optimizer_mean.zero_grad()\n",
    "    optimizer_variance.zero_grad()\n",
    "    optimizer_head.zero_grad()\n",
    "    optimizer_embedding.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization of the model comes from adjusting the following 4 parameters:\n",
    "* **files**: changes how many playlists are loaded in *(located in the second code cell of the notebook)* \n",
    "* **learning rate**: how fast the weights change as it goes backwards along the gradient in the mlp *(located three code cells above)*  \n",
    "* **training set**: how many traning sets the model runs through *(located below)*  \n",
    "* **epochs**: how many times the whole process is repeated *(located below)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle training, we have the two parameters epochs and training_sets. Epochs determines how many times the full model is run, and the training_sets determines how many of the 10 training sets it runs through per epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2, Sets: 10\n",
      "\n",
      "epoch 0\n",
      "set 1, set 2, set 3, set 4, set 5, set 6, set 7, set 8, set 9, set 10, \n",
      "epoch 1\n",
      "set 1, set 2, set 3, set 4, set 5, set 6, set 7, set 8, set 9, set 10, \n"
     ]
    }
   ],
   "source": [
    "# currently, if either of these are set to one, the loss.backwards() function gives an error\n",
    "training_sets = 10 \n",
    "epochs = 2\n",
    "\n",
    "print('Epochs: {}, Sets: {}'.format(epochs, training_sets))\n",
    "print()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('epoch ' + str(epoch))\n",
    "    for i in range(1 , training_sets + 1):\n",
    "        train(emb_play, i)\n",
    "        print('set {},'.format(i), end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our model currently relies on the \"retain_grad\" flag on the loss.backward() function, we believe this is causing errors. Unfortunately, we couldn't find a way around it. Because of this, the songs recommended come out looking strange, as explained in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function shows the top 15 songs that the model recommends. It takes in the index of the plalist (index 25 is a country playlist). It shows what each training set had as it's training target, the recommended songs, and the confidence the model calculated for the recommended songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training target 1: Caught Up In The Moment - Big & Rich  \n",
      "Training target 2: 8th Of November - Album Version w/o Intro - Big & Rich  \n",
      "Training target 3: Save A Horse (Ride A Cowboy) - Big & Rich  \n",
      "Training target 4: Rollin' (The Ballad Of Big & Rich) - The Ballad Of Big & Rich Album Version - Big & Rich  \n",
      "Training target 5: Holy Water - Big & Rich  \n",
      "Training target 6: Something in the Water - Carrie Underwood  \n",
      "Training target 7: Cowboy Casanova - Carrie Underwood  \n",
      "Training target 8: One Way Ticket - Carrie Underwood  \n",
      "Training target 9: Thank God For Hometowns - Carrie Underwood  \n",
      "Training target 10: Girl In A Country Song - Maddie & Tae  \n",
      "\n",
      "0.000095, that one song - gnash  \n",
      "0.000095, Just Might Be - Young Thug  \n",
      "0.000095, Closer - The Chainsmokers  \n",
      "0.000094, Rolex - Ayo & Teo  \n",
      "0.000094, Love Me Down - Britney Spears  \n",
      "0.000094, Then - Brad Paisley  \n",
      "0.000094, Breaking Point - Crown The Empire  \n",
      "0.000094, Ghost - Oliver Nelson Remix - Ella Henderson  \n",
      "0.000093, Into The Black - Chromatics  \n",
      "0.000093, The Waiting One - All That Remains  \n",
      "0.000093, Ghostbusters - Ray Parker, Jr.  \n",
      "0.000093, Come Thru - Drake  \n",
      "0.000093, There’s A Girl - Trent Harmon  \n",
      "0.000092, Special (feat. Offset and Solo Lucci) - Young Thug  \n",
      "0.000092, Island Party - Tropixx  \n"
     ]
    }
   ],
   "source": [
    "def show_recommendation(playlist_index):\n",
    "    top_fifteen = get_recommendations(playlist_index, emb_play).softmax(dim=0).topk(15)\n",
    "\n",
    "    recommend = [[tracks[song], ] for song in top_fifteen.indices]\n",
    "    probability = [prop for prop in top_fifteen.values]\n",
    "    print('Training target 1: ' + tracks[playlists_train1[playlist_index].item()] + '  ')\n",
    "    print('Training target 2: ' + tracks[playlists_train2[playlist_index].item()] + '  ')\n",
    "    print('Training target 3: ' + tracks[playlists_train3[playlist_index].item()] + '  ')\n",
    "    print('Training target 4: ' + tracks[playlists_train4[playlist_index].item()] + '  ')\n",
    "    print('Training target 5: ' + tracks[playlists_train5[playlist_index].item()] + '  ')\n",
    "    print('Training target 6: ' + tracks[playlists_train6[playlist_index].item()] + '  ')\n",
    "    print('Training target 7: ' + tracks[playlists_train7[playlist_index].item()] + '  ')\n",
    "    print('Training target 8: ' + tracks[playlists_train8[playlist_index].item()] + '  ')\n",
    "    print('Training target 9: ' + tracks[playlists_train9[playlist_index].item()] + '  ')\n",
    "    print('Training target 10: ' + tracks[playlists_train10[playlist_index].item()] + '  ')\n",
    "    print()\n",
    "    for song, prob in zip(recommend, probability):\n",
    "        print('{:.6f}, {}  '.format(prob.item(), song[0]))\n",
    "\n",
    "show_recommendation(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further validate the model, we searched through the playlists to find ones that had easily-distinguishable genres for us to manually validate the genre of the songs being recommended.\n",
    "\n",
    "Here is a country playlist we found in position 25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_playlist(num):\n",
    "    raw_playlist = playlist_list[num][:]\n",
    "    for i, song in enumerate(raw_playlist):\n",
    "        raw_playlist[i] = tracks[song]\n",
    "    return raw_playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Drink A Beer - Luke Bryan',\n",
       " 'Crash My Party - Luke Bryan',\n",
       " 'Country Girl (Shake It For Me) - Luke Bryan',\n",
       " 'Drunk On You - Luke Bryan',\n",
       " \"That's My Kind Of Night - Luke Bryan\",\n",
       " \"If Heaven Wasn't So Far Away - Justin Moore\",\n",
       " 'Cowboys and Angels - Dustin Lynch',\n",
       " \"Where It's At - Dustin Lynch\",\n",
       " 'Little Toy Guns - Carrie Underwood',\n",
       " 'Two Black Cadillacs - Carrie Underwood',\n",
       " 'Good Girl - Carrie Underwood',\n",
       " 'Blown Away - Carrie Underwood',\n",
       " 'I Drive Your Truck - Lee Brice',\n",
       " 'Drinking class - Lee Brice',\n",
       " \"I Don't Dance - Lee Brice\",\n",
       " \"Beachin' - Jake Owen\",\n",
       " 'Barefoot Blue Jean Night - Jake Owen',\n",
       " 'Cop Car - Keith Urban',\n",
       " 'Somewhere In My Car - Keith Urban',\n",
       " 'Six Foot Town - Big & Rich']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_playlist(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also planned on using these playlists for our validating:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example playlist(7) - Rap/Hip-hop**\n",
    "\n",
    "Training target 1: I Don't Fuck With You - Big Sean  \n",
    "Training target 2: Drowning (feat. Kodak Black) - A Boogie Wit da Hoodie  \n",
    "Training target 3: goosebumps - Travis Scott  \n",
    "Training target 4: STFU - mansionz  \n",
    "Training target 5: Exposed - Russ  \n",
    "Training target 6: Slippery (feat. Gucci Mane) - Migos  \n",
    "Training target 7: m.A.A.d city - Kendrick Lamar  \n",
    "Training target 8: Yellow - Aminé  \n",
    "Training target 9: Juke Jam (feat. Justin Bieber & Towkio) - Chance The Rapper  \n",
    "Training target 10: Bodak Yellow - Cardi B\n",
    "\n",
    "**Example playlist(83) - Christmas**\n",
    "\n",
    "Training target 1: Run Rudolph Run - Single Version - Chuck Berry  \n",
    "Training target 2: Santa Claus Is Comin' to Town - Single Version - Bruce Springsteen  \n",
    "Training target 3: Linus And Lucy - Vince Guaraldi Trio  \n",
    "Training target 4: Christmas Time Is Here - Vocal - Vince Guaraldi Trio  \n",
    "Training target 5: Do They Know It's Christmas? - 1984 Version - Band Aid  \n",
    "Training target 6: All I Want for Christmas Is You - Mariah Carey  \n",
    "Training target 7: Santa Claus Go Straight To The Ghetto - James Brown  \n",
    "Training target 8: Joy To The World - Sufjan Stevens  \n",
    "Training target 9: Little Drummer Boy/Silent Night/Auld Lang Syne - Extended Version - Jimi Hendrix  \n",
    "Training target 10: Christmas All Over Again - Tom Petty and the Heartbreakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we did not get to use these selected playlists in our validation because our model was unsuccessful. As it trains, it gains more and more confidence in its top 15 songs, but those top 15 songs do not change for different playlists. It seems that the longer it trains, the less it distinguishes between playlists and the more confidence it has in those 15 songs. The reason for this behavior is unknown, but we assume it has to do with the `loss.backward()` function while we train. We currently have to use `retain_graph=True` to avoid errors, and we assume geting rid of this flag would let the model run properly. So, our validation process is useless in the face of our model's current outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An earlier version of our model concatenated the mean, variance, and target song in order to train, rather than going through mean and variance sequentially. We moved away from this architecture once we realized that gradient information was not retained when we did this because concatenating tensors loses the gradient information in them. So our first improvement was to change models.\n",
    "\n",
    "When we changed to the new model, we initially fed it a song that was meant to go into the playlist to help it along in training, and when we did this, we tried to optimize by changing four parameters - amount of data loaded in, learning rate, training sets used, and epochs.\n",
    "\n",
    "Eventually, we realized that the end result of the model could not accept a song that is supposed to go into the playlist because the whole point is to recommend a song using only the playlist itself. Unfortunately, once we made this adjustment, the model never successfully recommended songs again. It recommended the same sets of songs for every playlist. By our deadline, we had not figured out a solution and did not have a meaningful way to optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other ways we could have set up our sequential model. We could have set up the mlp to calculate the output directly instead of calculating the change in its input. We also could have added more or less layers. We took into account both mean and variance, but it could be viable to use only one.  \n",
    "\n",
    "We also could have used PyTorch's model of an LSTM to make our model, but the documentation online turned out to be sorely confusing and unhelpful. For someone who has experience using the PyTorch LSTM, this could possibly be a better way to run our architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, our model did not perform the way we intended it. It recommends similar songs for every playlist, and the longer it trains, the more confident it is in its favorite 15 songs, no matter what the playlist is.  \n",
    "\n",
    "For future work, it would be necessary to have greater research and methods on how to loop through backpropogation without remembering the gradiant graph. In the 90 minutes of research the team did, we could not find the answer to this question. Since we have found no other flaw in the model architecture, our conclusion is that this problem is the cause of our model's poor recommendations. And unlimited time, memory, and GPU could lead us to a successful model in the near future."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8f4b6a4c38f2085446f7f3f791a848eb62f1444c2c1b7234c3e9244444f9b89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
